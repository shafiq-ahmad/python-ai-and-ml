# Mix

Train test split matters

Support Vector Machine (SVM) Theory: SVM is a supervised learning algorithm that is used for classification. Types of SVM: Linear, Polynomial, RBF, Sigmoid

K-Nearest Neighbors (KNN) Theory: KNN is a supervised learning algorithm that is used for classification. KNN is a lazy learning algorithm that is used for classification. Formula for KNN: (x1-x2)^2 + (y1-y2)^2.

Euclidean Distance: The distance between two points in a Euclidean space is the square root of the sum of the squares of the differences of the coordinates of the points. Formula: (x1-x2)^2 + (y1-y2)^2. Where x1, x2, y1, y2 are the coordinates of the points.

Manhattan Distance: The distance between two points in a Manhattan space is the sum of the absolute differences of the coordinates of the points. Formula: |x1-x2| + |y1-y2|. Where x1, x2, y1, y2 are the coordinates of the points.

Minkowski Distance: Minkowski distance is the distance between two points in a Minkowski space. Formula: (x1-x2)^p + (y1-y2)^p. Where x1, x2, y1, y2 are the coordinates of the points.

Hamming Distance: Hamming distance is the number of bits that are different between two strings. Formula: |x1-x2| + |y1-y2|. Where x1, x2, y1, y2 are the strings.

## Algorithms

- Support Vector Machine (SVM): Fr linear and non linear data. Useful for regression and classification, also outlier detection.
- K-Nearest Neighbors (KNN): based on distance. also work for non normal data.
- Linear Regression: simple linear regression, multiple linear regression.
- Logistic Regression: for linear and classified.


## Decision Tree:

Decision Tree based on decision making. like true/false. It starts from root, stem, branckes, subbranches, and ends on leafs. Node is a point where data split. Root is top node. Splitting is process of spliting a node into two nodes. Decision node is the node that is used to make a decision. Leaf node or terminal is node which is no more splits, it represents the final result / decision.

Pruining: removing the small nodes.

Entropy measure: is the measure of impurity. or randomness. Formula for entropy: -p*log2p. Entropy (s) = -∑p(i)log2p(i)
Information gain: is the measure of impurity. or randomness. Formula for information gain: -p*log2p + (1-p)*log2(1-p)
Gini measure: is the measure of impurity. or randomness. Formula for gini: 1-∑p(i)^2



